{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader as pdr\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from multiprocessing import *\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import *\n",
    "from numba import jit\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = './Data/train.csv'\n",
    "DATA_TEST_PATH =  './Data/test.csv'\n",
    "\n",
    "def load_data(train_path = DATA_TRAIN_PATH, test_path = DATA_TEST_PATH):\n",
    "    train = pd.read_csv(train_path, na_values = \"-1\", dtype = np.float64)\n",
    "    train = train.fillna(-1)\n",
    "    test = pd.read_csv(test_path, na_values = \"-1\")\n",
    "    test = test.fillna(-1)\n",
    "    \n",
    "    x_train = train.drop(['target', 'id'], axis = 1)\n",
    "    y_train = train['target']\n",
    "    id_train = train['id'].values\n",
    "    print('Train data shape: ', x_train.shape)\n",
    "    \n",
    "    x_test = test.drop(['id'], axis = 1)\n",
    "    id_test = test['id'].values\n",
    "    print('Test data shape: ', x_test.shape)   \n",
    "    \n",
    "    return x_train, y_train, id_train, x_test, id_test\n",
    "\n",
    "def feature_info(df):\n",
    "    cat_features = df.columns[df.columns.str.endwith('cat')].tolist()\n",
    "    bin_features = df.columns[df.columns.str.endwith('bin')].tolist()\n",
    "    num_features = [feature for feature in df.columns.tolist() \n",
    "                    if feature not in cat_features and features not in bin_features]\n",
    "    return cat_features, bin_features, num_features\n",
    "\n",
    "def submit(filename, id_test, pred_test):\n",
    "    sub = pd.DataFrame()\n",
    "    sub['id'] = id_test\n",
    "    sub['target'] = pred_test\n",
    "    sub.to_csv(filename, index = False)\n",
    "\n",
    "def gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_normalized(y_true, y_pred):\n",
    "    return gini(y_true, y_pred)/gini(y_true, y_true)\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return[('gini', gini_score)]\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return 'gini', gini_score, True    \n",
    "    \n",
    "def add_noise(series, noise_level):\n",
    "    return series*( 1+ noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def rfe_svc(x_train, y_train):\n",
    "    estimator = SVC(kernel='rbf', C=1, verbose = True)    \n",
    "    #rfe = RFE(estimator = estimator, n_features_to_select=5, step = 1, verbose = 1)    \n",
    "    rfe = RFECV(estimator = estimator, step = 1, cv = 5 , verbose = 1)\n",
    "    rfe = rfe.fit(x_train, y_train)    \n",
    "    return rfe.ranking_, rfe.support_\n",
    "\n",
    "def rfe_gb(x_train, y_train):\n",
    "    estimator = ExtraTreesClassifier(n_estimators = 50, max_depth=None, min_samples_split=2, random_state=0, verbose=1)    \n",
    "    estimator = estimator.fit(x_train, y_train)\n",
    "    model = SelectFromModel(estimator, prefit = True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (595212, 57)\n",
      "Test data shape:  (892816, 57)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, id_train, x_test, id_test = load_data(train_path = DATA_TRAIN_PATH, test_path = DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking, support = rfe_svc(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_model = rfe_gb(x_train, y_train)\n",
    "x_train = pd.DataFrame(rfe_model.transform(x_train))\n",
    "x_test =  pd.DataFrame(rfe_model.transform(x_test))\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 5\n",
    "stratkfold = StratifiedKFold(n_splits=kfold, random_state=0, shuffle = True)\n",
    "y_pred=[]\n",
    "num_round = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# params ={'learning_rate' : 0.02,\n",
    "#          'max_depth' : 6,\n",
    "#          'max_bin' : 10,\n",
    "#          'feature_fraction' : 0.9,\n",
    "#          'bagging_fraction' : 0.9,\n",
    "#          'bagging_frequency': 10,\n",
    "#          'min_data' : 500,\n",
    "#          'objective' : 'binary',\n",
    "#          'metric' : 'auc',\n",
    "#          'bagging_seed' : 99\n",
    "# }\n",
    "\n",
    "   \n",
    "params ={'learning_rate' : 0.02,\n",
    "     'max_depth' : 10,\n",
    "     'max_bin' : 20,\n",
    "     'feature_fraction' : 0.9,\n",
    "     'bagging_fraction' : 0.9,\n",
    "     'bagging_frequency': 10,\n",
    "     'min_data' : 500,\n",
    "     'objective' : 'binary',\n",
    "     'metric' : 'auc',\n",
    "     'bagging_seed' : 99\n",
    "}\n",
    "    \n",
    "for i, (train_index, test_index) in enumerate(stratkfold.split(x_train, y_train)):   \n",
    "    print(' light gbm kfold: {}  of  {} : '.format(i+1, kfold))    \n",
    "    d_train = lgb.Dataset(x_train.iloc[train_index], label = y_train.iloc[train_index]) \n",
    "    d_valid = lgb.Dataset(x_train.iloc[test_index], label = y_train.iloc[test_index]) \n",
    "\n",
    "    model = lgb.train(params, d_train, num_round, d_valid, early_stopping_rounds = 50, \n",
    "                  feval = gini_lgb, verbose_eval = 100)\n",
    "    y_pred.append(model.predict(x_test, num_iteration=(model.best_iteration+50)))\n",
    "\n",
    "model.save_model('lgb_model_'+ str(k) + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'eta': 0.02, \n",
    "#           'max_depth': 5, \n",
    "#           'subsample': 0.9, \n",
    "#           'colsample_bytree': 0.9, \n",
    "#           'objective': 'binary:logistic', \n",
    "#           'eval_metric': 'auc', \n",
    "#           'seed': 99, \n",
    "#           'silent': True}\n",
    "\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(stratkfold.split(x_train, y_train)):\n",
    "#     print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))    \n",
    "#     d_train = xgb.DMatrix(x_train.iloc[train_index], y_train.iloc[train_index]) \n",
    "#     d_valid = xgb.DMatrix(x_train.iloc[test_index], y_train.iloc[test_index]) \n",
    "#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    \n",
    "#     model = xgb.train(params, d_train, num_round, watchlist, early_stopping_rounds = 50, \n",
    "#                   feval = gini_xgb, maximize = True, verbose_eval = 10)\n",
    "\n",
    "#     y_pred.append(model.predict(xgb.DMatrix(x_test), ntree_limit=(model.best_ntree_limit+50)))\n",
    "\n",
    "# pickle.dump(model, open(\"xgb_model.pickle.dat\", \"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = np.mean(y_pred, axis=0)\n",
    "submit('xgb1_mean.csv', id_test, y_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
